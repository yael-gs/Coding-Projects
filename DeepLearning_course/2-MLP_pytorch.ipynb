{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KFU-_ZrsZFR"
   },
   "source": [
    "# Multi-Layer Perceptron for binary-classification / in pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-0LCM7sseew"
   },
   "source": [
    "## Objective:\n",
    "\n",
    "The objective of this lab is to develop a two hidden layers MLP to perform **binary classification**.\n",
    "\n",
    "We will use a MLP with 2 hidden layer with $n_{h1}=20$ and $n_{h2}=10$ hidden units and ```relu``` activation functions.\n",
    "You will perform 10.000 iterations (epochs) of SGD to find the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhcaeREyshiA"
   },
   "source": [
    "### Data normalization\n",
    "\n",
    "You should normalize the data to zero mean and unit standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W889TJY0sjrw"
   },
   "source": [
    "### Model\n",
    "\n",
    "There are various ways to write NN model in pytorch.\n",
    "\n",
    "We will write three different implementations:\n",
    "- **Model A**: manually defining the parameters (W1,b1,W2,b2,W3,b3), writing the forward equations, writting the loss equation, calling the .backward() and manually updating the weights using W1.grad.\n",
    "- **Model B**: using the Sequential class of pytorch\n",
    "- **Model C**: using a custom torch.nn.Module class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Qc8jENGsmR5"
   },
   "source": [
    "### Loss\n",
    "\n",
    "Since we are dealing with a binary classification problem, we will use a Binary Cross Entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbkGTDaRso25"
   },
   "source": [
    "### Parameters update/ Optimization\n",
    "\n",
    "For updating the parameters, we will use as optimizer a simple SGD algorithm with a learning rate of 0.1.\n",
    "\n",
    "Reminder : The optimizer is applied to a set of parameters.\n",
    "\n",
    "Once the gradients have been computed (after the backpropagation has been performed), we perform one step of optimization (either by hand or with optimizer.step())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qzL47Ntss3p"
   },
   "source": [
    "### Backward propagation\n",
    "\n",
    "Backpropagation is automatically performed in pytorch using the ```autograd``` package.  \n",
    "First, reset the gradients of all parameters then perform the backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4fnzJJDo60Y"
   },
   "source": [
    "### Documentations:\n",
    "\n",
    "- Introduction to pytorch\n",
    "\n",
    "    - https://perso.telecom-paristech.fr/gpeeters/video/pytorch.mp4\n",
    "    - https://perso.telecom-paristech.fr/gpeeters/doc/pytorch/#1\n",
    "\n",
    "- Specific pytorch packages\n",
    "\n",
    "    - NN: https://pytorch.org/docs/stable/nn.html\n",
    "    - Autograd: https://pytorch.org/docs/stable/autograd.html\n",
    "    - Optim: https://pytorch.org/docs/stable/optim.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuvU8y2Lo60Z"
   },
   "source": [
    "## Load the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1726754926818,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "I1VTuwVio60a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsb-phrJo60g"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "We take the usual circle dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1726754927090,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "Otam7ukPo60g"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X_np, y_np = datasets.make_circles(n_samples=1000, noise=0.2, factor=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5idAV4Co60i"
   },
   "source": [
    "We convert the ```numpy tensors``` to ```torch tensors```.\n",
    "The difference being that the latters allows to do automatic gradient differentiation (back-propagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1726754927090,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "rPxnzVSDo60j"
   },
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_np).float()\n",
    "y = torch.from_numpy(y_np).float()\n",
    "y = y.view(len(y), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1726754927091,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "YHgd8JYPo60l",
    "outputId": "1a4d0a82-1d81-47ce-bf9f-ff0f39839326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 2])\n",
      "torch.Size([1000, 1])\n",
      "tensor([-0.0133, -0.0021])\n",
      "tensor([0.6044, 0.5994])\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())\n",
    "print(X.mean(dim=0))\n",
    "print(X.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unp-3kjjo60n"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1726754927091,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "6rIwFaauo60n",
    "outputId": "a98ae916-5af7-43ab-c6f1-ef1ecc47c2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3351e-08,  1.5497e-08])\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "X -= X.mean(dim=0)\n",
    "X /= X.std(dim=0)\n",
    "print(X.mean(dim=0))\n",
    "print(X.std(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rc3VgVWOo60p"
   },
   "source": [
    "## Definition of the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1726754927341,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "OrBQMqCJo60r"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m n_in \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m n_h1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;66;03m#first layer is 20 units\u001b[39;00m\n\u001b[0;32m      3\u001b[0m n_h2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;66;03m#second layer is 10 units\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "n_in = X.shape[1]\n",
    "n_h1 = 20 #first layer is 20 units\n",
    "n_h2 = 10 #second layer is 10 units\n",
    "n_out = 1 #binary classifation\n",
    "\n",
    "nb_epoch = 10000\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIzjntgZo60t"
   },
   "source": [
    "## Model 1 (writing the network equations)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pllj-HFth04"
   },
   "source": [
    "### Parameter initialization\n",
    "\n",
    "**Using torch.Tensors**.\n",
    "we define the variables and write the equations of the network ourself.\n",
    "However we will use ```torch.Tensors``` instead of ```numpy.Array```.\n",
    "\n",
    "***Why ?*** because torch tensors allow us to **automatically** get the gradient.  \n",
    "Simply using ```loss.backward()``` to launch the backpropagation from ```loss```.  \n",
    "Then, for all tensors we've created and for which we declared ```requires_grad=True```, we will get the gradient of ```loss```with respect to this variable in the field ```.grad```.\n",
    "\n",
    "Example: ```W1 = torch.Tensor(..., requires_grad=True)``` ... ```loss.backward``` will have the gradient $\\frac{d Loss}{d W1}$in ```W1.grad```.\n",
    "\n",
    "**Random initialization**. The weight $W_1, W_2, \\cdots$ matrices **should be initialized randomly with small values**; while the bias vectors $b_1, b_2, \\cdots$can be initialized to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1726754927342,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "fGEMW1P1tllj"
   },
   "outputs": [],
   "source": [
    "# --- We first initialize the variables of the network (W1, b1, ...)\n",
    "\n",
    "W1 = torch.rand(n_in, n_h1)/100\n",
    "W1.requires_grad = True\n",
    "b1 = torch.zeros(n_h1).T\n",
    "b1.requires_grad = True\n",
    "\n",
    "W2 = torch.rand(n_h1, n_h2)/100\n",
    "W2.requires_grad = True\n",
    "b2 = torch.zeros(n_h2).T\n",
    "b2.requires_grad = True\n",
    "\n",
    "W3 = torch.rand(n_h2, n_out)/100\n",
    "W3.requires_grad = True\n",
    "b3 = torch.zeros(n_out).T\n",
    "b3.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GU472VdktqoP"
   },
   "source": [
    "### Define the model\n",
    "\n",
    "We write a function to perform the forward pass (using pytorch operators, not numpy operators) taking X as input and returing hat_y as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1726754927342,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "1BWc4OoZts_l"
   },
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    A0 = X\n",
    "    Z1 = A0 @ W1 + b1\n",
    "    A1 = torch.relu(Z1)\n",
    "    Z2 = A1 @ W2 + b2\n",
    "    A2 = torch.relu(Z2)\n",
    "    Z3 = A2 @ W3 + b3\n",
    "    A3 = torch.sigmoid(Z3)\n",
    "    hat_y = A3\n",
    "    \n",
    "    return hat_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-t--05ydty4g"
   },
   "source": [
    "### Main training loop\n",
    "\n",
    "We iterate over epochs (we do not perform split into mini-batch here).\n",
    "For each iteration, we\n",
    "- a) perform the forward pass,\n",
    "- b) compute the loss/cost,\n",
    "- c) compute the backward pass to get the gradients of the cost w.r.t. the parameters W1, b1, ...\n",
    "- d) perform the update of the parameters W1, b1, ...\n",
    "\n",
    "***Important: pytorch restriction***  \n",
    "When we update the parameters (W1, b1, ...), we muist do that ``inplace`` (W1 -= ??? and not W1 = W1 - ???).  \n",
    "**Why?** Because otherwise pytorch will overide W1 by its value, and hence **it will eras its .grad file** used to backpropagate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10656,
     "status": "ok",
     "timestamp": 1726754937989,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "HB65rqW4o60u",
    "outputId": "7d3e2ecb-2a61-4268-c56b-6a918321f24e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, nb_epoch):\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# --- a) Forward pass: X (n_in, m), hat_y (n_out, m)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     hat_y \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# -- We clip hat_y in order to avoid log(0) in the loss\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "for num_epoch in range(0, nb_epoch):\n",
    "\n",
    "    # --- a) Forward pass: X (n_in, m), hat_y (n_out, m)\n",
    "    hat_y = model(X)\n",
    "\n",
    "    # -- We clip hat_y in order to avoid log(0) in the loss\n",
    "    eps = 1e-10\n",
    "    hat_y = torch.clamp(hat_y, eps, 1-eps)\n",
    "\n",
    "    # --- b) Computing the loss/cost\n",
    "    loss = - ( y*torch.log(hat_y) + (1-y)*torch.log(1-hat_y)) ##binary cross entropy\n",
    "    cost = torch.sum(loss)/X.size()[0]\n",
    "\n",
    "\n",
    "    if num_epoch % 500 == 0:\n",
    "        print('epoch {}, loss {}'.format(num_epoch, cost))\n",
    "\n",
    "    # --- c) Backward pass\n",
    "    cost.backward()\n",
    "\n",
    "    # --- \"with torch.no_grad()\" temporarily set all the requires_grad flag to false\n",
    "    with torch.no_grad():\n",
    "        # --- d) perform the update of the parameters W1, b1, ...\n",
    "        # --- the gradients dLoss/dW1 is stored in W1.grad, dLoss/db1 is stored in b1.grad, ...\n",
    "        W1 -= W1.grad * alpha\n",
    "        b1 -= b1.grad * alpha\n",
    "        W2 -= W2.grad * alpha\n",
    "        b2 -= b2.grad * alpha\n",
    "        W3 -= W3.grad * alpha\n",
    "        b3 -= b3.grad * alpha\n",
    "\n",
    "\n",
    "    # --- We need to set to zero all gradients (otherwise they are cumulated)\n",
    "    W1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    W2.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "    W3.grad.zero_()\n",
    "    b3.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxgA4DXVo60w"
   },
   "source": [
    "## Model 2 (using nn.sequential)\n",
    "\n",
    "Here, we use the package ```torch.nn``` which comes with a predefined set of layers. The syntax is close to the one of ```keras```(```Sequential```), but differs in the fact that layers are splitted into the matrix multiplication followed by a non-linear activations (```keras```merge both using the ```Dense```layers).\n",
    "\n",
    "The model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```. It is therefore a convenient way to write simple sequential networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 256,
     "status": "ok",
     "timestamp": 1726755335786,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "elGQpQzjo60x"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m my_model \u001b[38;5;241m=\u001b[39m  torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[0;32m      2\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(n_in, n_h1),\n\u001b[0;32m      3\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m      4\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(n_h1,n_h2),\n\u001b[0;32m      5\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(n_h2, n_out),\n\u001b[0;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSigmoid(),\n\u001b[0;32m      8\u001b[0m     )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "my_model =  torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_in, n_h1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(n_h1,n_h2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(n_h2, n_out),\n",
    "    torch.nn.Sigmoid(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ts4aVeIPo601"
   },
   "source": [
    "### Criterion and Optimization\n",
    "\n",
    "The code of Model 1 is self-contained, i.e. it already contains all necessary instruction to perform forward, loss, backward and parameter updates.\n",
    "\n",
    "When using ```nn.sequential``` (model 2) or a class definition of the network (model 3), we still need to define\n",
    "- what we will minimize (the loss to be minimized, i.e. Binary-Cross-Entropy). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)\n",
    "- how we will minimize the loss, i.e. what parameter update algorithms we will use (SGD, momentum). We can of course write the equation of it by hand but pytorch comes with a very large number of pre-build loss functions (within ```torch.nn```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1726755354757,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "xs63V-Wgo602"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89ysZn6rw17u"
   },
   "source": [
    "### Main training loop\n",
    "\n",
    "Having defined the network, the citerion to be minimized and the optimizer, we then perform a loop over epochs (iterations); at each step we\n",
    "- compute the forward pass by passing the data to the model: ```haty = model(x)```\n",
    "- compute the the loss (the criterion)\n",
    "- putting at zero the gradients of all the parameters of the network (this is important since, by default, pytorch accumulate the gradients over time)\n",
    "- computing the backpropagation (using as before ```.backward()```)\n",
    "- performing one step of optimization (using ```.step()```)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "executionInfo": {
     "elapsed": 10762,
     "status": "ok",
     "timestamp": 1726755367354,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "JROT567kw17u",
    "outputId": "66a2ddf9-9ded-4119-b483-d283ab5fccc6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m loss_l \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m num_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nb_epoch):\n\u001b[0;32m      4\u001b[0m     hat_y \u001b[38;5;241m=\u001b[39m my_model(X) \u001b[38;5;66;03m# Forward pass: Compute predicted y by passing  x to the model\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(hat_y,y) \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "loss_l = []\n",
    "for num_epoch in range(nb_epoch):\n",
    "\n",
    "    hat_y = my_model(X) # Forward pass: Compute predicted y by passing  x to the model\n",
    "    loss = criterion(hat_y,y) # Compute loss\n",
    "\n",
    "    # re-init the gradients (otherwise they are cumulated)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # perform back-propagation (which will update the gradients)\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_l.append(loss.item())\n",
    "\n",
    "    if num_epoch % 500 == 0:\n",
    "        print('epoch {}, loss {}'.format(num_epoch, loss.item()))\n",
    "\n",
    "# ----------------\n",
    "plt.plot(loss_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4twpnbEAo60z"
   },
   "source": [
    "## Model 3 (using a class definition)\n",
    "\n",
    "Here, you will write the network using the recommended pytroch way; i.e. by defining a class.\n",
    "This class inherit from the main class ```torch.nn.Module```.\n",
    "You only need to write the ```__init__``` method and the ```forward``` method.\n",
    "\n",
    "In object programming, the ```__init__``` method defines the attributes of your class. Since the attributes of your  network are the parameters to be trained (weights and biases), you should declare in the ```__init``` all the layers that involve parameters to be trained (mostly the ```Linear```layers which perform the matrix multiplication).\n",
    "\n",
    "The ```forward``` method contains the code of the forward pass itself. It can of course call attributes defined in the ```__init___``` method. It is the method used when calling ```model(x)```.\n",
    "\n",
    "As before, the model created will have all its parameters accessible as a dictionary and can be accessed using ```model.parameters()```.\n",
    "\n",
    "Classes are convenient way to write more complex network than what you can do with ```nn.sequential```. Note that you can actually include a ```nn.sequential``` in your class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1726756539492,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "ZNsNkq9Do60z"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_h1, n_h2, n_out):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        if student:\n",
    "            # --- START CODE HERE (06)\n",
    "            self.fc1 = nn.Linear(n_in, n_h1) # hidden layer 1\n",
    "            self.fc2 = nn.Linear(n_h1, n_h2) # hidden layer 2\n",
    "            self.fc3 = nn.Linear(n_h2, n_out)  # output layer\n",
    "            # --- END CODE HERE\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        if student:\n",
    "            # --- START CODE HERE (07)\n",
    "            A0 = X\n",
    "            A1 = torch.relu(self.fc1(A0))   # activation function for hidden layer 1\n",
    "            A2 = torch.relu(self.fc2(A1))   # activation function for hidden layer 2\n",
    "            A3 = torch.sigmoid(self.fc3(A2))   # activation function for output layer\n",
    "            # --- END CODE HERE\n",
    "\n",
    "        return A3\n",
    "\n",
    "# --- START CODE HERE\n",
    "my_model = Net(n_in, n_h1, n_h2, n_out)\n",
    "# --- END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfOoKPMtwfLI"
   },
   "source": [
    "### Criterion and Optimization (same as for model 2 -> copy and paste your code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1726756556676,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "e-DC96kOwfLJ"
   },
   "outputs": [],
   "source": [
    "if student:\n",
    "    # --- START CODE HERE (08)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.SGD(my_model.parameters(),lr=0.1)\n",
    "    # --- END CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XamuBM_ho604"
   },
   "source": [
    "### Main training loop (same as for model 2 -> copy and paste your code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "executionInfo": {
     "elapsed": 10295,
     "status": "ok",
     "timestamp": 1726756568678,
     "user": {
      "displayName": "Yaël -",
      "userId": "00491070160584580396"
     },
     "user_tz": -120
    },
    "id": "rKfrD8V3o605",
    "outputId": "3404270f-491e-4bb3-e7cf-7dc8ed261997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 0.6970490217208862\n",
      "epoch 500, loss 0.30917608737945557\n",
      "epoch 1000, loss 0.2692456841468811\n",
      "epoch 1500, loss 0.26258087158203125\n",
      "epoch 2000, loss 0.2603543996810913\n",
      "epoch 2500, loss 0.2594291567802429\n",
      "epoch 3000, loss 0.25883516669273376\n",
      "epoch 3500, loss 0.2583462595939636\n",
      "epoch 4000, loss 0.25795912742614746\n",
      "epoch 4500, loss 0.25772109627723694\n",
      "epoch 5000, loss 0.25753268599510193\n",
      "epoch 5500, loss 0.2573728859424591\n",
      "epoch 6000, loss 0.2572425901889801\n",
      "epoch 6500, loss 0.25711724162101746\n",
      "epoch 7000, loss 0.256918340921402\n",
      "epoch 7500, loss 0.2567879557609558\n",
      "epoch 8000, loss 0.25659507513046265\n",
      "epoch 8500, loss 0.2563868463039398\n",
      "epoch 9000, loss 0.25623080134391785\n",
      "epoch 9500, loss 0.25608643889427185\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_l = []\n",
    "for num_epoch in range(nb_epoch):\n",
    "\n",
    "    if student:\n",
    "        # --- START CODE HERE (09)\n",
    "        hat_y = my_model(X) # Forward pass: Compute predicted y by passing  x to the model\n",
    "        loss = criterion(hat_y,y) # Compute loss\n",
    "\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "\n",
    "        # re-init the gradients (otherwise they are cumulated)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # perform back-propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # --- END CODE HERE\n",
    "\n",
    "    loss_l.append(loss.item())\n",
    "\n",
    "    if num_epoch % 500 == 0:\n",
    "        print('epoch {}, loss {}'.format(num_epoch, loss.item()))\n",
    "\n",
    "# ----------------\n",
    "plt.plot(loss_l);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1NyxJ60WuIfQDg8TXGCOoFddmYD2b6uDQ",
     "timestamp": 1726747464806
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS = False\n",
    "trade_plots = False \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_path = \"D:\\Data-cfm\\X_train.csv\"\n",
    "y_train_path = \"D:\\Desktop\\Coding-Projects\\Prediction-Challenge\\Data\\y_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = pd.read\n",
    "# y_train = pd.read_csv(test_path)\n",
    "# y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data = 504 days × 24 stocks × 20 observations/day × 100 events/observation  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a description of each column in the dataset. <br>\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ------------ |\n",
    "| **Obs_id** | which observation are we taking into account <br>-> for that observation we will keep track of the next 100 operations in the book orders |\n",
    "| **Venue_id** | for a given stock, exchanges can happen across many venues :  this id tracks which venue we consider <br> ==> it could be of importance (some stocks are typically traded across many venues ?) |\n",
    "| **order_id** | for a given observation sequence, each operation is related to an order. An order can be added, updated, deleted. <br>The order_id allows to track the lifecycle of individual orders within a sequence.   |\n",
    "| **action** |  A (adding an order to the book) , D (Deleting an order from the book), U = updating an action from the book |\n",
    "| **side** | B (bids, values to buy the action) , A (Ask, values to sell the action) \n",
    "| **Price** | - price : price of the order that was affected. *This best_bid_price , at the time of the first event, is substracted from all price reated columns (price, bid, ask  ) |\n",
    "| **bid , ask** |- bid , ask == best bid (highest bid) /best ask (lowest ask)   |\n",
    "| **bid_size, ask_size** |  volume of orders at the best bid, respectively ask, price  , on the *aggregated book* <br> => this too could be a valuable information, perhaps some stocks are encoutering more volume than others.  |\n",
    "|**flux** | the change in volume at a specific price level in the order book due to a particular event |\n",
    "|**Trade**|A boolean true or false to indicate whether a deletion or update event was due to a trade or due to a cancellation. <br> Most Deletions and updates actually dont occur from Trades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: For a given Observation\n",
    "\n",
    "| `order_id` | `action` | `price` | `side` | **Description**                                          |\n",
    "|------------|----------|---------|-------|----------------------------------------------------------|\n",
    "| 0          | A        | 100.5   | B     | A new order (ID 0) is added at 100.5 on the bid side.    |\n",
    "| 1          | A        | 101.0   | A     | A new order (ID 1) is added at 101.0 on the ask side.    |\n",
    "| 0          | U        | 100.5   | B     | The order with ID 0 is updated (e.g., quantity changed). |\n",
    "| 1          | D        | 101.0   | A     | The order with ID 1 is deleted (removed from the book).  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore trade info intuition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trade_plots:\n",
    "        \n",
    "    # Filter actions that are either 'D' or 'U'\n",
    "    du_actions = df[(df['action'].isin(['D', 'U']))]\n",
    "\n",
    "    # Count actions where 'trade' is True among 'D' or 'U'\n",
    "    du_trades = du_actions[du_actions['trade'] == True]\n",
    "\n",
    "    # Calculate the percentage\n",
    "    percentage = (len(du_trades) / len(du_actions)) * 100\n",
    "\n",
    "    # Display the result\n",
    "    print(f\"Percentage of 'D' or 'U' actions coming from trades: {percentage:.2f}%\")\n",
    "\n",
    "    # Merge the main DataFrame (df) with y_train using obs_id\n",
    "    df = df.merge(y_train, on='obs_id', how='left')  # Assuming y_train has columns ['obs_id', 'stock']\n",
    "\n",
    "    # Define a function to calculate the percentage for each observation\n",
    "    def calculate_percentage(sub_df):\n",
    "        is_du = sub_df['action'].isin(['D', 'U'])\n",
    "        is_du_trade = is_du & (sub_df['trade'] == True)\n",
    "        return (is_du_trade.sum() / is_du.sum()) * 100 if is_du.sum() > 0 else 0\n",
    "\n",
    "    # Group by obs_id and calculate percentage\n",
    "    df_obs = df.groupby('obs_id').apply(calculate_percentage).reset_index(name='percentage')\n",
    "\n",
    "    df_obs = df_obs.merge(y_train, on='obs_id', how='left')\n",
    "\n",
    "    # Group by stock and calculate statistics\n",
    "    stock_stats = df_obs.groupby('eqt_code_cat')['percentage'].agg(['mean', 'std', 'min', 'max'])\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.boxplot(x='eqt_code_cat', y='percentage', data=df_obs)\n",
    "    plt.title('Distribution of Percentages by Stock')\n",
    "    plt.xlabel('Stock')\n",
    "    plt.ylabel('Percentage of D/U Actions from Trades')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "        \n",
    "    # Cap percentages at 6%\n",
    "    df_obs_capped = df_obs[df_obs['percentage'] <= 6]\n",
    "\n",
    "    # Create subplots: One histogram per stock\n",
    "    stocks = df_obs_capped['eqt_code_cat'].unique()  # Get unique stocks\n",
    "    num_stocks = len(stocks)\n",
    "\n",
    "    # Define the number of rows and columns for subplots\n",
    "    ncols = 4  # Number of columns\n",
    "    nrows = (num_stocks + ncols - 1) // ncols  # Calculate rows based on number of stocks\n",
    "\n",
    "    # Create the figure\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, nrows * 3))\n",
    "    axes = axes.flatten()  # Flatten axes for easier indexing\n",
    "\n",
    "    # Plot each stock\n",
    "    for i, stock in enumerate(stocks):\n",
    "        # Filter the data for the current stock\n",
    "        stock_data = df_obs_capped[df_obs_capped['eqt_code_cat'] == stock]\n",
    "        \n",
    "        # Plot the histogram for the stock\n",
    "        sns.histplot(\n",
    "            stock_data,\n",
    "            x='percentage',\n",
    "            bins=30,\n",
    "            ax=axes[i],\n",
    "            element='step',\n",
    "            stat='percent'  # Show percentages instead of counts\n",
    "        )\n",
    "        axes[i].set_title(f'Stock {stock}', fontsize=12)\n",
    "        axes[i].set_xlim(0, 6)  # Cap the percentage at 6\n",
    "        axes[i].set_ylim(0, 100)  # Cap the y-axis at 100%\n",
    "        axes[i].set_xlabel('Percentage of D/U Actions', fontsize=10)\n",
    "        axes[i].set_ylabel('Percentage (%)', fontsize=10)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be three modes : 0% of trades , 2% of trades, 4% of trades "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some ideas after the initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given observation, what can help determine the stock ?    \n",
    "we could use visualisation (for a given stock : average volatility observed , average number of increase of orders, average number of decrease of oders etc simple metrics as such)  \n",
    "  \n",
    "To go more in depth : we must use embeddings of our data, think of interesting traits, use correlations, try and reduce the dimensionality.  \n",
    "--> ideas seem endless we could train an embedding matrix to predict the venue idk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing the Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark for the challenge is the following architecture :  \n",
    "\n",
    "Preprocess:  \n",
    "converting each event into a 30-dimensionnal vector.  \n",
    "group each 100-event-observations into a single \"observation\" vector, size 100x30\n",
    "  \n",
    "Architecture:  \n",
    "bidirectionnal GRU network, with 64 hidden units.  Producing a single 128 dimensional vector per \"observation vector\" .  \n",
    "Many to one architecture :converts the \"observation vector\" (of 100 individual events) into a single embedding of size 124.  \n",
    "Then two dense layers 124 -> 64 with SeLU activation, 64 -> 24 with softmax activation  \n",
    "  \n",
    "Training :  \n",
    "Cross entropy Loss  \n",
    "batch size : 1000 \"obervation vectors\"  (dim : 1000x100x30)  \n",
    "optimizer : Base ADAM with lr = 10e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing of the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact pre process structure isnt described, so I will do what sounds relevant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a description of each column in the dataset. <br>\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ------------ |\n",
    "| **Obs_id** | which observation are we taking into account <br>-> for that observation we will keep track of the next 100 operations in the book orders |\n",
    "| **Venue_id** | for a given stock, exchanges can happen across many venues :  this id tracks which venue we consider <br> ==> it could be of importance (some stocks are typically traded across many venues ?) |\n",
    "| **order_id** | for a given observation sequence, each operation is related to an order. An order can be added, updated, deleted. <br>The order_id allows to track the lifecycle of individual orders within a sequence.   |\n",
    "| **action** |  A (adding an order to the book) , D (Deleting an order from the book), U = updating an action from the book |\n",
    "| **side** | B (bids, values to buy the action) , A (Ask, values to sell the action) \n",
    "| **Price** | - price : price of the order that was affected. *This best_bid_price , at the time of the first event, is substracted from all price reated columns (price, bid, ask  ) |\n",
    "| **bid , ask** |- bid , ask == best bid (highest bid) /best ask (lowest ask)   |\n",
    "| **bid_size, ask_size** |  volume of orders at the best bid, respectively ask, price  , on the *aggregated book* <br> => this too could be a valuable information, perhaps some stocks are encoutering more volume than others.  |\n",
    "|**flux** | the change in volume at a specific price level in the order book due to a particular event |\n",
    "|**Trade**|A boolean true or false to indicate whether a deletion or update event was due to a trade or due to a cancellation. <br> Most Deletions and updates actually dont occur from Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#venue => one hot encode it \n",
    "#action => one hot encode it \n",
    "#side : => one hot encode it \n",
    "#price,bid,ask,bid_size,ask_size,flux : no transfo\n",
    "#trade : one hot encode it \n",
    "\n",
    "#Justifications ? => none, just exploring \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import h5py  # For saving large arrays in memory-efficient HDF5 format\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_df(df_to_encode):\n",
    "    categorical_columns = ['venue','action','side','trade']\n",
    "    df_pandas_encoded = pd.get_dummies(df_to_encode,columns=categorical_columns,drop_first=False,dtype=int)\n",
    "\n",
    "    return df_pandas_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_df(df):\n",
    "    #we want to drop obs_id, order_id \n",
    "    df = df.drop(['order_id','obs_id'],axis=1) #dropping obs id because they did so in the benchmark\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = df.head(int(10e3))\n",
    "# df_test = encode_df(df_test)\n",
    "# df_test = transform_df(df_test)\n",
    "# df_test\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_data(data, k):\n",
    "    '''\n",
    "    input:\n",
    "        data - the pandas object of (n_observations x 100 , p) shape, where n is the number of rows,\n",
    "               p is the number of predictors\n",
    "        k    - the length of the sequences, namely, the number of previous rows \n",
    "               (including current) we want to use to predict the target.\n",
    "    output:\n",
    "        X_data - the predictors numpy matrix of (n-k, k, p) shape\n",
    "    '''\n",
    "\n",
    "\n",
    "    # initialize zero matrix of (n-k, k, p) shape to store the n-k number\n",
    "    # of sequences of k-length and zero array of (n-k, 1) to store targets\n",
    "    X_data = np.zeros((data.shape[0]//k, k, data.shape[1]))\n",
    "    \n",
    "    # run loop to slice k-number of previous rows as 1 sequence to predict\n",
    "    # 1 target and save them to X_data matrix and y_data list\n",
    "    for i in range(data.shape[0]//k):\n",
    "        cur_sequence = data.iloc[k*i: k*(i+1), :]\n",
    "                \n",
    "        X_data[i,:,:] = cur_sequence\n",
    "    \n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(pd.read_csv(y_train_path).drop('obs_id',axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing a library to handle out-of-memeory packages https://stackoverflow.com/questions/30376581/save-numpy-array-in-append-mode/64403144#64403144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: npy-append-array in d:\\applications\\miniconda\\envs\\cfm\\lib\\site-packages (0.9.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install npy-append-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from npy_append_array import NpyAppendArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_data_chunked(x_train_path, y_train_path, output_prefix, chunk_size=10_000, seq_len=100):\n",
    "    \"\"\"\n",
    "    Process data chunk by chunk and save the results incrementally.\n",
    "    \n",
    "    Args:\n",
    "    - x_train_path: Path to the X_train CSV file.\n",
    "    - y_train_path: Path to the y_train CSV file.\n",
    "    - output_prefix: Prefix for the output files.\n",
    "    - chunk_size: Number of rows to process in each chunk.\n",
    "    - seq_len: Length of each sequence for LSTM.\n",
    "    \"\"\"\n",
    "    # Read y_train (the target file) entirely as it's small and doesn't need chunking\n",
    "    y_train_full = pd.read_csv(y_train_path).drop('obs_id', axis=1)\n",
    "\n",
    "    # Use tqdm to show progress\n",
    "    total_rows = sum(1 for _ in open(x_train_path)) - 1  # Get total rows excluding header\n",
    "    num_chunks = (total_rows + chunk_size - 1) // chunk_size  # Calculate total chunks\n",
    "\n",
    "    X_train_npy_name = f\"{output_prefix}_X_train.npy\"\n",
    "    y_train_npy_name = f\"{output_prefix}_y_train.npy\"\n",
    "\n",
    "\n",
    "    # Process the X_train file in chunks\n",
    "    with NpyAppendArray(X_train_npy_name, delete_if_exists=True) as npaa:\n",
    "        for i, chunk in enumerate(tqdm(pd.read_csv(x_train_path, chunksize=chunk_size), desc=\"Processing Chunks\", total=num_chunks)):\n",
    "            # Apply the transformation functions\n",
    "            chunk = transform_df(encode_df(chunk))\n",
    "\n",
    "            # Create LSTM-compatible data for this chunk\n",
    "            X_data = create_lstm_data(chunk, seq_len)\n",
    "\n",
    "            npaa.append(X_data)\n",
    "\n",
    "            # Clear memory for the current chunk\n",
    "            del X_data, chunk\n",
    "\n",
    "\n",
    "    np.save(y_train_npy_name,y_train_full)\n",
    "    print(f\"Processing completed. X_train and y_train saved as {output_prefix}_X_train.npy and {output_prefix}_y_train.npy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 1608/1608 [01:52<00:00, 14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. X_train and y_train saved as 27-11_X_train.npy and 27-11_y_train.npy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# process_data_chunked(x_train_path, y_train_path, '27-11', chunk_size=10_000, seq_len=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = r\"D:\\Desktop\\Coding-Projects\\Prediction-Challenge\\Notebooks\\27-11_X_train.npy\"\n",
    "\n",
    "# # Load the .npy file\n",
    "# data = np.load(file_path)\n",
    "\n",
    "# # Display the data (e.g., shape, a sample of the contents)\n",
    "# print(\"Data Shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_path = r\"D:\\Desktop\\Coding-Projects\\Prediction-Challenge\\Notebooks\\27-11_y_train.npy\"\n",
    "# # Load the .npy file\n",
    "# y = np.load(y_path)\n",
    "\n",
    "# # Display the data (e.g., shape, a sample of the contents)\n",
    "# print(\"Data Shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "160800/10050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs,x_path_npy,y_path_npy, batch_size=10050, dim=(100,19),\n",
    "                 n_classes=24, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        self.x_path = x_path_npy\n",
    "        self.y_path = y_path_npy\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim))\n",
    "        y = np.empty((self.batch_size))\n",
    "\n",
    "        # Generate data\n",
    "        X_full = np.load(self.x_path, mmap_mode=\"r\")\n",
    "        y_full = np.load(self.y_path,mmap_mode=\"r\")\n",
    "\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = X_full[ID]\n",
    "\n",
    "            # Store class\n",
    "            y[i] = y_full[ID].astype(int)[0]\n",
    "        \n",
    "        del X_full\n",
    "        del y_full\n",
    "        \n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'dim': (100,19),\n",
    "          'batch_size': 10050,\n",
    "          'n_classes': 24,\n",
    "          'shuffle': True}\n",
    "\n",
    "# Datasets\n",
    "full_ids = np.arange(160800)\n",
    "\n",
    "# Shuffle the IDs to ensure randomness\n",
    "np.random.shuffle(full_ids)\n",
    "\n",
    "# Compute the split index for 80/20\n",
    "split_index = int(len(full_ids) * 13/16) #13/16 test, 3/16 val\n",
    "\n",
    "# Split the IDs\n",
    "train_ids = full_ids[:split_index]\n",
    "val_ids = full_ids[split_index:]\n",
    "\n",
    "x_path_npy = r'D:\\Desktop\\Coding-Projects\\Prediction-Challenge\\Notebooks\\27-11_X_train.npy'\n",
    "y_path_npy = r'D:\\Desktop\\Coding-Projects\\Prediction-Challenge\\Notebooks\\27-11_y_train.npy'\n",
    "\n",
    "# Generators\n",
    "training_generator = DataGenerator(train_ids,x_path_npy,y_path_npy, **params)\n",
    "val_generator = DataGenerator(val_ids,x_path_npy,y_path_npy, **params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data in a satisfactory format.  \n",
    "each row of our X_train is made of 100 event, each of these events is represented in a 18 dimension space.  \n",
    "and for each row of our train set, we have a single target value : in y  \n",
    "Let's now create a similar architecture as the benchmark  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Bidirectional,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 19)]         0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              43008     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                1560      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 52,824\n",
      "Trainable params: 52,824\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Input for fixed-length (length = 100) sequences of event observation (dimension = 19)\n",
    "inputs = keras.Input(shape=(100,19))\n",
    "\n",
    "# Add 2 bidirectional LSTMs\n",
    "x = Bidirectional(LSTM(64))(inputs)\n",
    "x = Dense(64)(x)\n",
    "# Add a classifier\n",
    "outputs = Dense(24, activation=\"softmax\")(x)\n",
    "model_2 = keras.Model(inputs, outputs)\n",
    "model_2.summary()\n",
    "\n",
    "model_2.compile(optimizer=keras.optimizers.Adam(learning_rate=3e-3), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) #need sparsecatcrossentropy cause integer labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "\n",
    "# Define callbacks to enhance and monitor the training process\n",
    "callbacks = [\n",
    "    # 1. ModelCheckpoint:\n",
    "    # Saves the model to a file ('best_model.h5') whenever the validation loss ('val_loss') improves.\n",
    "    # Ensures that only the best version of the model (with the lowest validation loss) is saved.\n",
    "    ModelCheckpoint(\n",
    "        filepath='best_model.h5',   # Filepath to save the model\n",
    "        monitor='val_loss',        # Metric to monitor\n",
    "        save_best_only=True,       # Save only the best model\n",
    "        mode='min'                 # Minimize the 'val_loss'\n",
    "    ),\n",
    "    \n",
    "    # 2. EarlyStopping:\n",
    "    # Stops training if the validation loss does not improve for 'patience' epochs (5 in this case).\n",
    "    # Prevents overfitting and saves time by stopping early when progress stalls.\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',        # Metric to monitor\n",
    "        patience=5,                # Number of epochs to wait without improvement\n",
    "        mode='min',                # Minimize the 'val_loss'\n",
    "        restore_best_weights=True  # Restore the model weights from the best epoch\n",
    "    ),\n",
    "    \n",
    "    # 3. ReduceLROnPlateau:\n",
    "    # Reduces the learning rate when the validation loss plateaus (does not improve for 3 epochs here).\n",
    "    # Helps the model converge better by lowering the learning rate when progress slows down.\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',        # Metric to monitor\n",
    "        factor=0.2,                # Factor by which to reduce the learning rate\n",
    "        patience=3,                # Number of epochs to wait before reducing the learning rate\n",
    "        min_lr=1e-6                # Minimum learning rate to avoid reducing it too much\n",
    "    ),\n",
    "    \n",
    "    # 4. TensorBoard:\n",
    "    # Logs training metrics, such as loss and accuracy, for visualization using TensorBoard.\n",
    "    # Also logs histograms and the computational graph of the model.\n",
    "    TensorBoard(\n",
    "        log_dir='./logs',          # Directory to save TensorBoard logs\n",
    "        histogram_freq=1,          # Log histograms of weights after every epoch\n",
    "        write_graph=True,          # Save the computation graph\n",
    "        write_images=True          # Save visualizations of weights and biases\n",
    "    ),\n",
    "    \n",
    "    # 5. CSVLogger:\n",
    "    # Logs training and validation metrics to a CSV file ('training_log.csv').\n",
    "    # Useful for tracking metrics over time and for external analysis.\n",
    "    CSVLogger(\n",
    "        filename='training_log.csv',  # Path to save the log file\n",
    "        append=True                  # Append to existing file if it exists\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "13/13 [==============================] - 317s 21s/step - loss: 3.1583 - accuracy: 0.0604 - val_loss: 3.0107 - val_accuracy: 0.0774 - lr: 0.0030\n",
      "Epoch 2/2\n",
      "13/13 [==============================] - 450s 34s/step - loss: 2.9522 - accuracy: 0.0840 - val_loss: 2.8996 - val_accuracy: 0.0895 - lr: 0.0030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x218fdbe64f0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model on dataset\n",
    "model_2.fit(\n",
    "    x=training_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=2,  # Specify the number of epochs as needed\n",
    "    callbacks = callbacks\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction with our trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "Top 3 predicted labels : [17  0  9]\n",
      "with following Top 3 values: [0.12209046 0.09839866 0.09341515]\n",
      "True Label 9\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "Top 3 predicted labels : [19 23  6]\n",
      "with following Top 3 values: [0.12720132 0.12183362 0.09871302]\n",
      "True Label 6\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "Top 3 predicted labels : [ 0 10  4]\n",
      "with following Top 3 values: [0.0731776 0.0698133 0.0617378]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "Top 3 predicted labels : [ 3 11  5]\n",
      "with following Top 3 values: [0.06901506 0.06339215 0.06177472]\n",
      "True Label 15\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "Top 3 predicted labels : [ 2 23  7]\n",
      "with following Top 3 values: [0.11479475 0.10407948 0.08718833]\n",
      "True Label 8\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Top 3 predicted labels : [ 0 17 22]\n",
      "with following Top 3 values: [0.14511058 0.13858934 0.12294507]\n",
      "True Label 9\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Top 3 predicted labels : [17  0 16]\n",
      "with following Top 3 values: [0.1603903  0.14310531 0.11755989]\n",
      "True Label 0\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Top 3 predicted labels : [ 0 22 17]\n",
      "with following Top 3 values: [0.15455785 0.137841   0.12589736]\n",
      "True Label 4\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Top 3 predicted labels : [5 8 7]\n",
      "with following Top 3 values: [0.09591497 0.09557791 0.08029601]\n",
      "True Label 1\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "Top 3 predicted labels : [20 14 19]\n",
      "with following Top 3 values: [0.08180721 0.07309849 0.06076501]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Top 3 predicted labels : [23  6  2]\n",
      "with following Top 3 values: [0.08245275 0.08075958 0.08031604]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Top 3 predicted labels : [18  4 10]\n",
      "with following Top 3 values: [0.06830768 0.06160657 0.05860635]\n",
      "True Label 20\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Top 3 predicted labels : [10 12 21]\n",
      "with following Top 3 values: [0.08918067 0.07252102 0.06955709]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Top 3 predicted labels : [18 17 10]\n",
      "with following Top 3 values: [0.09400732 0.09086159 0.08364577]\n",
      "True Label 8\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [10 14 11]\n",
      "with following Top 3 values: [0.06905064 0.05943225 0.05930304]\n",
      "True Label 13\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Top 3 predicted labels : [2 7 5]\n",
      "with following Top 3 values: [0.0813083  0.07558493 0.06812391]\n",
      "True Label 14\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [8 2 7]\n",
      "with following Top 3 values: [0.10589693 0.09496532 0.09199588]\n",
      "True Label 19\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Top 3 predicted labels : [23 19  6]\n",
      "with following Top 3 values: [0.1178351  0.10284597 0.07764672]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [ 4 18 17]\n",
      "with following Top 3 values: [0.09596612 0.09418563 0.08232248]\n",
      "True Label 6\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Top 3 predicted labels : [15  8  7]\n",
      "with following Top 3 values: [0.09020353 0.08298514 0.07864214]\n",
      "True Label 20\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Top 3 predicted labels : [14  7  5]\n",
      "with following Top 3 values: [0.07492429 0.07123241 0.06456766]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Top 3 predicted labels : [17  9 22]\n",
      "with following Top 3 values: [0.10324769 0.10185197 0.07584511]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [23 19 14]\n",
      "with following Top 3 values: [0.11478878 0.09264141 0.08372445]\n",
      "True Label 23\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Top 3 predicted labels : [ 0  9 22]\n",
      "with following Top 3 values: [0.15956451 0.12579577 0.11807168]\n",
      "True Label 16\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Top 3 predicted labels : [19 23  6]\n",
      "with following Top 3 values: [0.16726555 0.14579844 0.10758707]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Top 3 predicted labels : [11 20  5]\n",
      "with following Top 3 values: [0.08593333 0.0803117  0.07120766]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [17 18  0]\n",
      "with following Top 3 values: [0.18865262 0.13252673 0.11957064]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [19  7 13]\n",
      "with following Top 3 values: [0.1661807  0.15669239 0.12294383]\n",
      "True Label 23\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Top 3 predicted labels : [19 13  7]\n",
      "with following Top 3 values: [0.1352512 0.113719  0.1047964]\n",
      "True Label 23\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "Top 3 predicted labels : [ 5  1 12]\n",
      "with following Top 3 values: [0.07785664 0.07675777 0.07312655]\n",
      "True Label 11\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Top 3 predicted labels : [13 23  2]\n",
      "with following Top 3 values: [0.10993575 0.10963704 0.10936525]\n",
      "True Label 14\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Top 3 predicted labels : [22 18  4]\n",
      "with following Top 3 values: [0.14419451 0.1437135  0.13093954]\n",
      "True Label 20\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Top 3 predicted labels : [ 7  2 13]\n",
      "with following Top 3 values: [0.14945894 0.11626719 0.1140227 ]\n",
      "True Label 11\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Top 3 predicted labels : [23  6 19]\n",
      "with following Top 3 values: [0.14010292 0.13842964 0.12724224]\n",
      "True Label 6\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Top 3 predicted labels : [19  6 13]\n",
      "with following Top 3 values: [0.20121835 0.08459034 0.08216803]\n",
      "True Label 19\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Top 3 predicted labels : [19 13  6]\n",
      "with following Top 3 values: [0.2113865  0.17646497 0.13877603]\n",
      "True Label 19\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Top 3 predicted labels : [ 0  9 18]\n",
      "with following Top 3 values: [0.12148715 0.1202044  0.10035217]\n",
      "True Label 16\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Top 3 predicted labels : [21  4 20]\n",
      "with following Top 3 values: [0.08059556 0.06042369 0.05641045]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "Top 3 predicted labels : [13 19  7]\n",
      "with following Top 3 values: [0.12835562 0.09524216 0.08556501]\n",
      "True Label 6\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [2 6 1]\n",
      "with following Top 3 values: [0.08783428 0.08629952 0.0715339 ]\n",
      "True Label 23\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Top 3 predicted labels : [ 0 22 18]\n",
      "with following Top 3 values: [0.14232951 0.11824393 0.10771822]\n",
      "True Label 9\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Top 3 predicted labels : [ 7 13 23]\n",
      "with following Top 3 values: [0.12592424 0.12469576 0.10282344]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Top 3 predicted labels : [ 0 21 17]\n",
      "with following Top 3 values: [0.09873553 0.08633474 0.0836656 ]\n",
      "True Label 10\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "Top 3 predicted labels : [18  4  0]\n",
      "with following Top 3 values: [0.1304284  0.08727884 0.0796829 ]\n",
      "True Label 20\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Top 3 predicted labels : [13 23  7]\n",
      "with following Top 3 values: [0.102859   0.09404625 0.08098266]\n",
      "True Label 23\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Top 3 predicted labels : [21 10 20]\n",
      "with following Top 3 values: [0.1000124  0.08535499 0.07302742]\n",
      "True Label 15\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "Top 3 predicted labels : [21  9  4]\n",
      "with following Top 3 values: [0.09529694 0.07427856 0.06964716]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "Top 3 predicted labels : [ 0 17  9]\n",
      "with following Top 3 values: [0.14849463 0.1390728  0.12652814]\n",
      "True Label 9\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Top 3 predicted labels : [12 11  1]\n",
      "with following Top 3 values: [0.09272234 0.09010823 0.07058892]\n",
      "True Label 14\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Top 3 predicted labels : [ 2 13  8]\n",
      "with following Top 3 values: [0.12205967 0.10957693 0.08674295]\n",
      "True Label 7\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Top 3 predicted labels : [18  4 21]\n",
      "with following Top 3 values: [0.09648574 0.08745001 0.08007146]\n",
      "True Label 14\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Top 3 predicted labels : [5 3 2]\n",
      "with following Top 3 values: [0.09233474 0.06904002 0.0673803 ]\n",
      "True Label 8\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [ 1  5 11]\n",
      "with following Top 3 values: [0.0863524  0.07876223 0.070778  ]\n",
      "True Label 1\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top 3 predicted labels : [17  9  0]\n",
      "with following Top 3 values: [0.12525932 0.1081695  0.09132152]\n",
      "True Label 8\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Top 3 predicted labels : [ 9 22  0]\n",
      "with following Top 3 values: [0.10336601 0.0969042  0.08775914]\n",
      "True Label 18\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Top 3 predicted labels : [20 10  3]\n",
      "with following Top 3 values: [0.06700109 0.06420468 0.06158866]\n",
      "True Label 10\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Top 3 predicted labels : [ 0 17 22]\n",
      "with following Top 3 values: [0.16401313 0.15481116 0.13690393]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top 3 predicted labels : [23 15 11]\n",
      "with following Top 3 values: [0.07916316 0.07815243 0.07794204]\n",
      "True Label 12\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Top 3 predicted labels : [17  9  0]\n",
      "with following Top 3 values: [0.16886123 0.1197804  0.11091434]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Top 3 predicted labels : [12 15 11]\n",
      "with following Top 3 values: [0.07579858 0.07477427 0.06587556]\n",
      "True Label 8\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Top 3 predicted labels : [ 9  0 22]\n",
      "with following Top 3 values: [0.13611202 0.13575853 0.11119323]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Top 3 predicted labels : [23 13  7]\n",
      "with following Top 3 values: [0.09985953 0.09313104 0.08706726]\n",
      "True Label 13\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Top 3 predicted labels : [17  0 18]\n",
      "with following Top 3 values: [0.18229821 0.11437775 0.11188771]\n",
      "True Label 3\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Top 3 predicted labels : [10 15 20]\n",
      "with following Top 3 values: [0.06907581 0.05906859 0.05821553]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Top 3 predicted labels : [19 23  6]\n",
      "with following Top 3 values: [0.1782063  0.14331222 0.09538027]\n",
      "True Label 2\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Top 3 predicted labels : [10  4  3]\n",
      "with following Top 3 values: [0.06415889 0.05962988 0.05933027]\n",
      "True Label 3\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Top 3 predicted labels : [ 3  1 11]\n",
      "with following Top 3 values: [0.06810834 0.06206983 0.05923924]\n",
      "True Label 10\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "Top 3 predicted labels : [23  6  8]\n",
      "with following Top 3 values: [0.10961589 0.10742953 0.10153794]\n",
      "True Label 12\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Top 3 predicted labels : [15 23  7]\n",
      "with following Top 3 values: [0.08248892 0.07967116 0.0664621 ]\n",
      "True Label 2\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Top 3 predicted labels : [10 20  5]\n",
      "with following Top 3 values: [0.07237083 0.06796761 0.06375321]\n",
      "True Label 12\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Top 3 predicted labels : [19  6 23]\n",
      "with following Top 3 values: [0.12825301 0.12656963 0.08352914]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Top 3 predicted labels : [ 3  1 12]\n",
      "with following Top 3 values: [0.07494981 0.06809412 0.06595228]\n",
      "True Label 11\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Top 3 predicted labels : [ 4 21 22]\n",
      "with following Top 3 values: [0.09179641 0.08263926 0.07934415]\n",
      "True Label 18\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Top 3 predicted labels : [14 12  5]\n",
      "with following Top 3 values: [0.08356255 0.07782643 0.07707666]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Top 3 predicted labels : [17  0 22]\n",
      "with following Top 3 values: [0.14948077 0.12596303 0.10900626]\n",
      "True Label 17\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Top 3 predicted labels : [19 13 23]\n",
      "with following Top 3 values: [0.10702976 0.09975577 0.09724278]\n",
      "True Label 4\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top 3 predicted labels : [6 8 2]\n",
      "with following Top 3 values: [0.10029366 0.09247145 0.08417554]\n",
      "True Label 15\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Top 3 predicted labels : [12 19 14]\n",
      "with following Top 3 values: [0.10822326 0.08766764 0.08002967]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Top 3 predicted labels : [15  3 21]\n",
      "with following Top 3 values: [0.07586386 0.07491598 0.06435515]\n",
      "True Label 10\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Top 3 predicted labels : [ 1  3 15]\n",
      "with following Top 3 values: [0.07754715 0.0731846  0.06770348]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Top 3 predicted labels : [19  6 23]\n",
      "with following Top 3 values: [0.10005867 0.08480985 0.08099578]\n",
      "True Label 7\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "Top 3 predicted labels : [ 0 18  4]\n",
      "with following Top 3 values: [0.12358966 0.12351713 0.12283   ]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Top 3 predicted labels : [14 23 12]\n",
      "with following Top 3 values: [0.08083748 0.07186124 0.07003891]\n",
      "True Label 8\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Top 3 predicted labels : [19 13 14]\n",
      "with following Top 3 values: [0.21576217 0.10354734 0.08709319]\n",
      "True Label 19\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Top 3 predicted labels : [17 20  9]\n",
      "with following Top 3 values: [0.08387083 0.08318874 0.08073411]\n",
      "True Label 11\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Top 3 predicted labels : [ 2 23  3]\n",
      "with following Top 3 values: [0.08581005 0.07870067 0.06739887]\n",
      "True Label 0\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Top 3 predicted labels : [23 19  6]\n",
      "with following Top 3 values: [0.09175541 0.08811223 0.07882344]\n",
      "True Label 2\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Top 3 predicted labels : [ 5 11  7]\n",
      "with following Top 3 values: [0.08936338 0.0777874  0.06870574]\n",
      "True Label 5\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "Top 3 predicted labels : [ 4 10 18]\n",
      "with following Top 3 values: [0.07088692 0.06852952 0.06680807]\n",
      "True Label 21\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Top 3 predicted labels : [ 7 23 19]\n",
      "with following Top 3 values: [0.10311797 0.10048128 0.09651639]\n",
      "True Label 2\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "Top 3 predicted labels : [14 19  2]\n",
      "with following Top 3 values: [0.0966497  0.08130629 0.08026626]\n",
      "True Label 6\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "Top 3 predicted labels : [ 7 13 19]\n",
      "with following Top 3 values: [0.13331166 0.12452788 0.11995856]\n",
      "True Label 10\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Top 3 predicted labels : [ 7  3 14]\n",
      "with following Top 3 values: [0.08255255 0.06310934 0.06012056]\n",
      "True Label 16\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Top 3 predicted labels : [17 18  9]\n",
      "with following Top 3 values: [0.14692935 0.10755032 0.10343349]\n",
      "True Label 4\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Top 3 predicted labels : [23 19  6]\n",
      "with following Top 3 values: [0.09954973 0.09829824 0.08262848]\n",
      "True Label 3\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "Top 3 predicted labels : [ 4 22  0]\n",
      "with following Top 3 values: [0.12234442 0.09139602 0.0897993 ]\n",
      "True Label 22\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Top 3 predicted labels : [ 2 15 23]\n",
      "with following Top 3 values: [0.09770332 0.09003399 0.08581863]\n",
      "True Label 19\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Top 3 predicted labels : [23 11 12]\n",
      "with following Top 3 values: [0.08304732 0.07208414 0.07127007]\n",
      "True Label 10\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Top 3 predicted labels : [13  2  6]\n",
      "with following Top 3 values: [0.11933412 0.1028183  0.09298326]\n",
      "True Label 2\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "Top 3 predicted labels : [ 4 22 20]\n",
      "with following Top 3 values: [0.07967301 0.07941226 0.0705077 ]\n",
      "True Label 21\n"
     ]
    }
   ],
   "source": [
    "from random import  randint\n",
    "sequence = [randint(0,20000) for j in range(100)]\n",
    "\n",
    "#loading a sequence \n",
    "\n",
    "for i in sequence:\n",
    "        \n",
    "    A = np.load(x_path_npy,mmap_mode='r')[i] #this is a 100x19 sequence\n",
    "    A = np.expand_dims(A, axis=0)  # Adding batch dimension\n",
    "    A.shape\n",
    "    label = np.load(y_path_npy,mmap_mode='r')[i]\n",
    "\n",
    "    predicted = model_2.predict(A)\n",
    "\n",
    "    top_3_indices = predicted[0].argsort()[-3:][::-1]\n",
    "\n",
    "    # Get the top 3 values\n",
    "    top_3_values = predicted[0][top_3_indices]\n",
    "\n",
    "    print(\"Top 3 predicted labels :\", top_3_indices)\n",
    "    print(\"with following Top 3 values:\", top_3_values)\n",
    "\n",
    "    #print(f\"Label predicted\", predicted[0].argmax())\n",
    "    print(f\"True Label\",label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classes = np.load(y_path_npy)\n",
    "# pd.DataFrame(classes).describe()\n",
    "# #description of labels classes "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
